{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"aloyisius t mckeever  a new york city hobo makes his home in a boardedup fifth avenue mansion using the back door while its owner multimillionaire  michael j o'connor  winters in the south mckeever winds up taking in homeless exgi jim bullock  who has been evicted from an apartment building o'connor is tearing down for a new skyscraper and runaway yearold trudy smith  who unknown to him is o'connor's daughter soon jim invites war buddies whitey  hank  and their families to share the vast mansion while they seek permanent homes of their own trudy falls in love with jim and when her father demands to meet him convinces o'connor to also take up residence pretending to be the panhandler mike to win jim's love without the temptation of marrying her for her wealth mckeever allows mike to move in but treats him as a servant when mike warns trudy that he intends to have them all arrested for criminal trespass she persuades her mother mary  to fly up from florida and pretend to be the th interloper a cook mike has one of his construction companies bribe jim with a great job offer in bolivia but jim turns it down to pursue his dream the exgis have an idea to buy a former army camp and convert its barracks into inexpensive family housing unbeknownst to either side mike and jim get into a bidding war for the camp the gi's are trying to buy which mike wants for an air cargo terminal in the meantime mike and mary reconcile when she believes he has changed all are celebrating christmas eve together forgetting to hide as usual from the patrolmen who check the house every night but the patrolmen agree to let the families stay until after the new year jim reveals that he and his partners have lost the camp to michael j o'connor and when mike later defends his business dealings to mary she tells him he hasn't changed after all his dream shattered a depressed jim takes the job offer in bolivia breaking off with trudy mary and trudy angrily tell mike they are leaving for florida the next day because of the way he has manipulated jim mckeever is also depressed and tells mike he is going to find another mansion to live in next winter mike spins a tale that he has arranged a meeting with o'connor for jim and his partners who are dubious but accept they are astounded to learn that mike is o'connor he transfers the camp to them on the condition that they never reveal his true identity to mckeever that night everyone is reunited and share a celebratory dinner before putting the house back just the way we found it they see the stillunaware mckeever off as he heads to the o'connors' mansion in virginia and mike tells mary that next year mckeever will be coming in through the front door\",\n",
       " 'the documentary goes behind the scenes in the recording studio with arif mardin it shows him working with artists such as norah jones and jewel the documentary also features artists such as aretha franklin and producers such as george martin reflecting on his life and career it also explores his personality and humorous nature',\n",
       " \"diya  is an intelligent university student who does not agree with arranged marriage she spends much of her time at an orphanage in coorg helping her uncle  with the kids living there the story begins when diya travels to mumbai to take exams while there she stays with her father's friend and meets their lighthearted son arjun  his views on love are completely opposite hers but they start spending time with each other and soon fall in love arjun however is unaware that he is in love whereas diya professes her love misunderstandings occur until arjun ventures off to the orphanage in hopes of putting a smile on her face again after breaking her heart there he realizes his true feelings for diya who is planning to marry her childhood friend ishaan  luckily this was all just a plan 'uncle' made to get arjun to realize his mistake of not professing love for diya at the right moment\",\n",
       " \"set in the lively village of modern day punjab dil apna punjabi is about a family spanning over four generations all living under one roof headed by sardar hardam singh  his grandson kanwal  is a man of his heart who spends most of his time with his friends a village musical troupe when kanwal when meets college friend ladi  at relative faujan's  home he falls in love faujan makes their love match seem as an arranged marriage to their respective families however ladi's family meet him they are discouraged due to his unambitious approach and his lack of employment when a talent scout  hears him singing kanwal decides to make a success of himself in the uk to prove himself here he meets tv host lisa  lisa is drawn towards kanwal's charm and simplicity soon begins to have feelings for kanwal kanwal has to choose between fame and fortune with lisa in the uk or returning to his roots in the punjab to be with his first love ladi\",\n",
       " \"the original script by ilya tilkin has not any literary source the screenwriter studied diaries of the participants of the battle of stalingrad he also used museum's archives and documents and recorded the stories of its participants the plot is based on a dramatic love story against the backdrop of a grand battle the action takes place in  when the german troops occupied the bank of the volga river having failed while attempting to cross the volga and launch a counteroffensive on the german army the soviet troops were forced to retreat however a few soldiers managed to get to the shore of the enemy they remain in the minority and hide in a coastal house where they met a girl the germans occupied her home and she did not have time to leave the front line against the backdrop of the most bloody battle in the history of mankind develops a love story and from that moment the soldiers have to protect this girl at any cost the prototype of this house is the legendary pavlov's house in stalingrad and its history on the eve of the filming the script was significantly rewritten by the director and screenwriter sergey snezhkin including the plot and dialogues\"]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in = open(\"./data/plots_text.pickle\", \"rb\")\n",
    "movie_plots = pickle.load(pickle_in)\n",
    "\n",
    "movie_plots = [re.sub(\"[^a-z' ]\", \"\", i) for i in movie_plots]\n",
    "random.sample(movie_plots, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length:  152644\n"
     ]
    }
   ],
   "source": [
    "def create_seq(text, seq_len=5):\n",
    "    sequences = []\n",
    "    \n",
    "    if len(text.split()) > seq_len:\n",
    "        for i in range(seq_len, len(text.split())):\n",
    "            seq = text.split()[i-seq_len: i+1]\n",
    "            sequences.append(\" \".join(seq))\n",
    "    \n",
    "        return sequences\n",
    "    else:\n",
    "        return [text]\n",
    "\n",
    "seqs = [create_seq(p) for p in movie_plots]\n",
    "seqs = sum(seqs, [])\n",
    "print(\"dataset length: \",len(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "    word_list = s.split()\n",
    "    x.append(\" \".join(word_list[:-1]))\n",
    "    y.append(\" \".join(word_list[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'private'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "vocab = set(\" \".join(movie_plots).split())\n",
    "for w in vocab:\n",
    "    int2token[cnt] = w\n",
    "    cnt += 1\n",
    "\n",
    "token2int = {t: i for i, t in int2token.items()}\n",
    "vocab_size = len(vocab)\n",
    "int2token[token2int['private']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda string: [token2int[w] for w in string.split()]\n",
    "\n",
    "x_int = [encode(i) for i in x]\n",
    "y_int = [encode(i) for i in y]\n",
    "\n",
    "x_int = np.array(x_int)\n",
    "y_int = np.array(y_int)\n",
    "\n",
    "def get_batches(x, y, batch_size):\n",
    "    prv = 0\n",
    "    for n in range(batch_size, x.shape[0], batch_size):\n",
    "        output_x = x[prv:n,:]\n",
    "        output_y = y[prv:n,:]\n",
    "        prv = n\n",
    "        yield output_x, output_y \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"forward pass of the network\n",
    "        input through embedding layer (200) => feed init hidden state and input to lstm layer \n",
    "        => get output and new hidden => reshape output after drop out and feed to fully connected layer\n",
    "\n",
    "        Args:\n",
    "            x (_type_): _description_\n",
    "            hidden (_type_): _description_\n",
    "        \"\"\"\n",
    "        embedded = self.emb_layer(x)\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        out = self.drop_out(lstm_output)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(16592, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (drop_out): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=16592, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = WordLSTM()\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    net.to(device)\n",
    "    \n",
    "    counter = 0\n",
    "    net.train()\n",
    "    total_loss=0\n",
    "    for e in range(epochs):\n",
    "        hidden = net.init_hidden(batch_size)\n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter += 1\n",
    "            \n",
    "            inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, hidden = net(inputs, hidden)\n",
    "            loss = criterion(output, targets.view(-1).long())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "        print(\"Epoch: {}/ {} \".format(e+1, epochs), \"Loss: {:.4f}\".format(total_loss/counter))\n",
    "        total_loss = 0\n",
    "        counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/ 10  Loss: 7.0018\n",
      "Epoch: 2/ 10  Loss: 6.2330\n",
      "Epoch: 3/ 10  Loss: 5.7888\n",
      "Epoch: 4/ 10  Loss: 5.4962\n",
      "Epoch: 5/ 10  Loss: 5.2948\n",
      "Epoch: 6/ 10  Loss: 5.1359\n",
      "Epoch: 7/ 10  Loss: 5.0043\n",
      "Epoch: 8/ 10  Loss: 4.8875\n",
      "Epoch: 9/ 10  Loss: 4.7953\n",
      "Epoch: 10/ 10  Loss: 4.7203\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, tkn, h=None):\n",
    "    x = np.array([[token2int[tkn]]])\n",
    "    inputs = torch.from_numpy(x).to(device)\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    out, h = net(inputs, h)\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    p = p.cpu()\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "    \n",
    "    top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "    # randomly select one of the three indices\n",
    "    sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return int2token[sampled_token_index], h\n",
    "\n",
    "def sample(net, size, prime=''):\n",
    "        \n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "        token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i not be a vampire but is a cheat but she is aware of his grade and the us monster and'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sample(net, 20, 'i')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
