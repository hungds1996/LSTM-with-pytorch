{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Libraries required \n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "text = open(\"./data/data.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(35514,), dtype=int64, numpy=array([22, 57,  2, ..., 57, 42, 10], dtype=int64)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'I' b't' b' ' b'w' b'a' b's' b' ' b't' b'h' b'e' b' ' b'b' b'e' b's'\n",
      " b't' b' ' b'o' b'f' b' ' b't' b'i' b'm' b'e' b's' b',' b'\\n' b'i' b't'\n",
      " b' ' b'w' b'a' b's' b' ' b't' b'h' b'e' b' ' b'w' b'o' b'r' b's' b't'\n",
      " b' ' b'o' b'f' b' ' b't' b'i' b'm' b'e' b's' b',' b'\\n' b'i' b't' b' '\n",
      " b'w' b'a' b's' b' ' b't' b'h' b'e' b' ' b'a' b'g' b'e' b' ' b'o' b'f'\n",
      " b' ' b'w' b'i' b's' b'd' b'o' b'm' b',' b'\\n' b'i' b't' b' ' b'w' b'a'\n",
      " b's' b' ' b't' b'h' b'e' b' ' b'a' b'g' b'e' b' ' b'o' b'f' b' ' b'f'\n",
      " b'o' b'o' b'l'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'It was the best of times,\\nit was the worst of times,\\nit was the age of wisdom,\\nit was the age of fool'\n",
      "b'ishness,\\nit was the epoch of belief,\\nit was the epoch of incredulity,\\nit was the season of Light,\\nit '\n",
      "b'was the season of Darkness,\\nit was the spring of hope,\\nit was the winter of despair,\\nwe had everythin'\n",
      "b'g before us,\\nwe had nothing before us,\\nwe were all going direct to Heaven,\\nwe were all going direct t'\n",
      "b'he other way--\\nin short, the period was so far like the present period, that some of\\nits noisiest aut'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 10s 2s/step - loss: 4.4832\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 3.8583\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 3.8541\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 3.5477\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 9s 2s/step - loss: 3.2339\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 3.0370\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 2.9428\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 7s 2s/step - loss: 2.8800\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 2.8021\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 2.7181\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 7s 2s/step - loss: 2.6482\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 2.5832\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 2.5396\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 7s 1s/step - loss: 2.4963\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 7s 1s/step - loss: 2.4584\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 7s 1s/step - loss: 2.4270\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 7s 1s/step - loss: 2.4031\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 7s 1s/step - loss: 2.3764\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 8s 2s/step - loss: 2.3465\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 7s 2s/step - loss: 2.3243\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: l\n",
      "cos pig singis the mis ter\" tongee. \"Lupaco fiwtreid mosd wing Lol frelin thivlt and af aiche ane strevend wakin-xesos, atere ?ofe\n",
      "'t otteecetp leo thel,\n",
      "sif\n",
      "thes lc, alpsy asug ram watlllisis, an ltolrat iped, wal.\n",
      "Nond enof.\"\n",
      "Wit hery fitro se lorere. \"nss ing te sor ive the werarith ting bopel, ived iflrnlwrobtr haet\n",
      "dase hith.\"\n",
      "Tulgiph anig ind Tis th ang clir  hsmitce giceilpa rarl olochede ond rate herren, faog aiseng Co g cuthers wid, He\" the pash wibumis ane, huad boit ouy Auchis the\n",
      "gady ousend waas h tirveng pid-du the dns foe fokt. \"\n",
      "?fe-sutt warovs and biuinged srebideacheng Tof ngre hoond.\"\n",
      "\n",
      "lav. Jofuve bladr ale, yoveantawd thectol! hersecaret ie ctrorye,\n",
      "the rlakinuss,, tees\n",
      "al: Min\n",
      "and toricgea had\n",
      "ito has, weatse\n",
      "Frpem.\"\n",
      "\"\n",
      "Thtead jivseavouve he nousee gom-the cpey oferon\"e\n",
      "cerherad bags wats, rissl fustlo theris er f at had waverdes foup-y\n",
      "ar,\n",
      "\"\n",
      "has the Dolomsacvellelo se trern atith yir ape keso thalloy toissenped sa.\"\"\"\n",
      "\n",
      "RTeaw, ot an tole th coed, tor harits Teas  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.4959146976470947\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.1590\n",
      "\n",
      "Epoch 1 Loss: 4.5992\n",
      "Time taken for 1 epoch 9.36 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 3.5791\n",
      "\n",
      "Epoch 2 Loss: 3.7408\n",
      "Time taken for 1 epoch 6.61 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 3.7111\n",
      "\n",
      "Epoch 3 Loss: 3.6005\n",
      "Time taken for 1 epoch 6.67 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 3.4459\n",
      "\n",
      "Epoch 4 Loss: 3.3687\n",
      "Time taken for 1 epoch 6.67 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 3.2317\n",
      "\n",
      "Epoch 5 Loss: 3.1095\n",
      "Time taken for 1 epoch 7.24 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 3.0158\n",
      "\n",
      "Epoch 6 Loss: 3.0035\n",
      "Time taken for 1 epoch 7.81 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 2.9491\n",
      "\n",
      "Epoch 7 Loss: 2.9161\n",
      "Time taken for 1 epoch 7.68 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 2.8883\n",
      "\n",
      "Epoch 8 Loss: 2.8521\n",
      "Time taken for 1 epoch 7.30 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 2.7963\n",
      "\n",
      "Epoch 9 Loss: 2.7602\n",
      "Time taken for 1 epoch 6.81 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 2.7279\n",
      "\n",
      "Epoch 10 Loss: 2.6853\n",
      "Time taken for 1 epoch 7.11 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}\n",
    "  \n",
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'ROMEO:-LBath, nicirsthadt. ro ke?\"\\n\\n(hee hid\"\\n\"rstedivet an\\'y ny fowsop pice der ad tye\\nef, figtite Lodevet ofs. \\nkis thete ths wers mosteringy ar itheig indingertsin hong, yas mome fon puwand thepas thad bu he huon, Jod yor the sritery sn I in oumly tre\\nthe din ras eut masewe biotert\\nlacltin bous no the reis tins\\nwoerd ef thou memscee gcetirn sf. \\natCpanlan wo, pad ronte\\noad toto tyetreid mot himes tor, fors\\nridxquabl solels\\n\\nothe Dis the hpis tnach swole?\"\\nT\\nithan,; \"a-windtricudat thyemt of beapln bouse son, ale Dweorlst ingonans in tonche tlem\\nLke af in t hop teast, ai, forh lman th so hes thelces fpist,, sun os mas mad.\\n)\\nI att uode nhend\"\\nShe hot, ixo than. Thes ane last of kon iskr tow wingwithiny Aa y afpthe rems wo. Jhe oom, bypa irtits int ant facd inde do ans apcendowis,\\nou he thacheay at tiuwis sas of asladl homiqugheaney s ile aglast Cheek hfak bed thecind roand ning arof thatrelva boncind of therp fo krerlden wiis\\nink\\nof eo storce tres wrove beed ifoth tane\\ndod (aven\\'in tit thi'\n",
      " b'ROMEO:n), Dan hay thed glouct\".\\n\\n_\\nTist\\nThet, alik s withe macm wacond and orlit rers ns of wather hnwassery oumde\\nlre lisethe hadenerin her wad Shy aping smris tuarsen, n. Indemein inkerr\\'n; wuls as-tas ale lout ims bid t aspe tof tor hor se-wip toacacthe patse cous, dar,, naset lore oy, Jistd phe loy pateerin st aus his bei the the an itupins sy sos ves haribe ancilclos, arel ss ch, ande Mas th dheo bit blsr, ffe\\nA sice-tudent od. Wipes, dold aer ond\\nrise,\\nilm hin, rn ant thed ang\\nwane thecs therup. Aweras sis \\natfomf\\ntoitund ist mad siithenk, core hachin ithendachid abemcnen sended seeny\\nthet is. theon halod, anecend sour ralzaond of lastre sithe.\\n\"\\nzereathegy neat orrqoud wise, th ef bo warighund foug,acod toe Oade s ango d, thaqafstar lf Hed lo wos afdr? anwac vaas af, wes and ro fitry. A che-cheam, of wag sumene coens the if bfacresoud allimut cofems of blad sed ondey, an whimits t had ced passemed\\n\\nerr bust, alinus the heand tho th the hand ou Tht pead, sol ind ferer wale acoest, Ipre'\n",
      " b'ROMEO: Iucolang enit daspe, tobobkom tue, t, thesed.\\n\"\\nJathe jlaagd, an bong\\norerid lond, tisteve ton for. Daol thtuy.\"\"\\n\"\"\\n-Hhe rinoudandlt wike, the lsiclo Frsoat hes there ss thestk ondill waleng iles lolunay ntee loungpolxistt onis linrinowcerond wos tof towal, buters, sthss on o sousthe ghe thass ard Fatssole fot soute rinighermad thy fre int wish erer, in t il inud ald raf. Ihe maag I fath, tone f at aange sitke, qurengediss ripl mist!\"\\n\\n\"beiblens.\\n\\n\\nndy arl bid,t, sont ersebererentingtiche seasd re her notseend tutt bingok the at uged ane thee, roa wadt ipvean, der ther boret\\nattelutoxen\\'s pant rera\\nthe ad nicye ses of fay d topad ring\\nithad tomt monen-. vaon yestelr and soucale wipmte hostenveter alilr ening an disove the suts be Minde yompinged, reasy theans and chain chon hod gher\\nof, bepr, fofnintt as-af hackey, iman ind thet ary sur fofreveasttir wupuuchon arow te soky if dely, Whieps civerYes thichligla hamef is\\nilil, jeids ind wousDe roed bose\\nlerasenge\\nwicute an si mond wipant'\n",
      " b'ROMEO:\\n_,?\\nN\\n\"lbe\\nifn alric, Spakinit bty watlindoa cope and wathe wolsot tomght mere En.  hare shon bet srestSoog tre wners bres. \\nnted nir wis, ant hed male aolld. Fe-killot.\\n\\nAk kenad srwwa cor at at otu cbol her, choin oor ghac-re!g Tey angond eim das poutingind hachers thone ness ialivees,,, yot wiss coant, unot ath\\no\\'keres.\\nThert, lmaus.\\n\\nNid couto thre tis Lhew, of and touthe hady the ffir hesmeasten way tnple furs, an pp outh\\nser, fand.\"\\n\\nAgony virseangth merde mas ailld brumlil, ha bers.\"\\n\\nqNo fouvemity eankend palrir, leadelluy tha ciser wat in tidun rlsur lveathese s aly lfaton maic, copur thet\\nte thesde if wis sel of therrey, atim-ons hint\\nbha\\nthe bly fithapar thed, and therd ofivea Deaker a cor this, sof tlit powitlen reiwl opade n bes tarocof ssroum, arf isey\\n\\nhrassd ane cof dof th ers t ce we hot lof-yimingudld al dreckexinklald of tirt. Afr at we pingror bea\" xout tha dtom; finis t bwas currcotpen woud sl bis hacpir\\nhat Sun end fass his matbe operor, thin-jausored ruakle Deit'\n",
      " b'ROMEO:,\\n, he lud, Satouthet erea-fon\\nhre ond oee, in eroso turcord inge, fad, t olot bok Mpoudicke\\nrt Masid gealednid, ink, way as ofser te\\'d to thongin lom shithey th outhes ar.\\nV, Shexton serr of atif Bt tor uen yem.\\n\"\\nMmCowed hes alpighpa rut tyt aithithears yogerlligg oogthe jupthe forerpr, bofsed no sas he berleunin loner\\nund rof te-ser th ors kerkevet, his;?\\nc\\nwad it bagy tof soikpas.\\nThol po the, heinengran lad, toryers avd thecoroirl hantol\\'nl.\"\\n\\nIl rowels tl.\\nLeware ristin and wad tory inded, old, that inla\\nthe bus anl\\'cast lutuveathe Jas ches, lais for pess and fas, aocenast\\nand e d afd wid in thave sisters Ins the we best; theed tit lippenans of thed naw. nesg, weaceredtit-turikn chit sotiding, tumas tleld bur, sebe\"s wim the cabther\\nwloutsis. \\nbit atar id she t lich of heres tartes pam, int tow rit Lherelind ofat af yorgn tor aco ro\\'d theag de fower fering\\nwa tots, fsad aot ars asoxtheren, ristipeds afpith-ithem\\'s guomike hat in bein dic if toe want on it mtor fumem\\nthe thaac-whe'], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.143528461456299\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
